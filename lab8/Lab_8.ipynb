{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b94b52c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/22 08:31:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# since everyone will be using cluster at the same time\n",
    "# let's make sure that everyone has resource. that is why \n",
    "# the configuration uses dynamic resource allocation and\n",
    "# maximum 1 executor \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"true\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"1\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdd1cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "songs_df = spark.read.load(\"./train_triplets.txt\",\n",
    "                     format=\"csv\", sep=\"\\t\", inferSchema=\"true\", \n",
    "                     header=\"false\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419499c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a95ac2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- play_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_df.createOrReplaceTempView(\"songs\")\n",
    "songs_df = songs_df.withColumnRenamed(\"_c0\", \"user\")\\\n",
    "                   .withColumnRenamed(\"_c1\", \"song\")\\\n",
    "                   .withColumnRenamed(\"_c2\", \"play_count\")\n",
    "songs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bcd8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "played_more_than_10_times = spark.sql(\"select song from songs where play_count > 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f869f93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2043582"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "played_more_than_10_times.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18010689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username = \"b80344d063b5ccb3212f76538f3d9e43d87dca9e\"\n",
    "played_by_user = spark.sql(f\"select song from songs where user=\\\"{username}\\\"\")\n",
    "played_by_user.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef5db83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(user='b80344d063b5ccb3212f76538f3d9e43d87dca9e')\n"
     ]
    }
   ],
   "source": [
    "first_ten_entries = spark.sql(f\"select user from songs limit 10\")\n",
    "print(first_ten_entries.collect()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"b80344d063b5ccb3212f76538f3d9e43d87dca9e\"\n",
    "played_by_user_more_than_10_times = spark.sql(f\"select song from songs where user=\\\"{username}\\\" and play_count > 10\")\n",
    "played_by_user_more_than_10_times.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5289c",
   "metadata": {},
   "source": [
    "## Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686796c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "business = spark.read.json(\"./yelp-dataset/yelp_academic_dataset_business.json\")\n",
    "reviews = spark.read.json(\"./yelp-dataset/yelp_academic_dataset_review.json\")\n",
    "users = spark.read.json(\"./yelp-dataset/yelp_academic_dataset_user.json\")\n",
    "business.createOrReplaceTempView(\"business\")\n",
    "reviews.createOrReplaceTempView(\"reviews\")\n",
    "users.createOrReplaceTempView(\"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select state, count(state) as count from business group by state order by count(state) desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c7df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(distinct(*)) from (\n",
    "    select explode(split(categories, \\\",\\s*\\\")) as category from business\n",
    ")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b31187",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select category, count(category) from \n",
    "    (\n",
    "        select explode(split(categories, \\\",\\s*\\\")) as category \n",
    "        from business where city=\\\"Phoenix\\\"\n",
    "    )\n",
    "group by category order by count(category) desc limit 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \n",
    "    count(*) as friend_count \n",
    "from \n",
    "    users \n",
    "where \n",
    "    size(split(friends, \\\",\\s*\\\")) > 1000\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6368ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "with business_ratings as (\n",
    "    select \n",
    "        business_id, year(to_date(date)) as year, avg(stars) as rating \n",
    "    from \n",
    "        reviews group by business_id, year(to_date(date))\n",
    "),\n",
    "business_2014 as (\n",
    "    select \n",
    "        business_id, rating \n",
    "    from \n",
    "        business_ratings \n",
    "    where \n",
    "        year=2014\n",
    "),\n",
    "business_2017 as (\n",
    "    select \n",
    "        business_id, rating \n",
    "    from \n",
    "        business_ratings where year=2017\n",
    ")\n",
    "select \n",
    "    business_2014.business_id, business_2014.rating, business_2017.rating \n",
    "from \n",
    "    business_2014 \n",
    "inner join \n",
    "    business_2017 \n",
    "on \n",
    "    business_2014.business_id=business_2017.business_id \n",
    "where \n",
    "    business_2017.rating < business_2014.rating \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3c563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "with last_review as (\n",
    "    select user_id, categories, max(to_date(date)) from reviews group by user_id  \n",
    ")\n",
    "select explode(split(friends, \\\",\\s*\\\")) as friend from last_review where category='Chinese restaurant'\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
